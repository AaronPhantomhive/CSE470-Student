{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me start with my own intnroduction to this appendix. \n",
    "\n",
    "It turns out that an incredibly successful way to approach computer vision problems is to say:\n",
    "> I have the _goal_ that I want to _optimize_, and I can express it via a _loss function_. \n",
    "> I want to choose some _parameters_ that _minimize my loss_, subject to some _constraints_ on thos parameters. \n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an _optimization_ problem, and you have likely seen them in your algorithms classes:\n",
    "- The classic _Rod Cutting Problem_ used Dynamic Programming to solve for the optimal cuts to _maximize revenue_ by selling peices of a rod. \n",
    "- The _Fractional Knapsack_ problem and the _Scheduling Problem_ use a _Gready_ technique to optimize the amount you can fit into a knapsack, or the number of meetings you can schedule in a room."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A much more common example of an optimization problem is simply to have some _loss function_, $L(\\mathbf{w})$ \n",
    "where $\\mathbf{w} \\in \\mathcal{R}^n$ is a vector of parameters to your system, and $L(\\mathbf{w})$ is a scalar values differentiable function.\n",
    "\n",
    "Not all problems fit this mold, but you will find that _many_ important problems do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very often, the think you will be optimizing is the _probability_ that an image matches some interpretation. \n",
    "In this case you will also need to understand some statistics; and you will generally minimize the _negative log likelihood_ of the observation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may or may not recall _Taylor's Theorem_ from calculus. \n",
    "\n",
    "It provides a formula to for a _local_ approximation to a $k$-times differentiable function using a $k^{th}$ order polynomial. \n",
    "\n",
    "A consequence of Taylor's theorem is that any twice differentiable function can be locally approximated by a polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(w-\\delta) = f(w) - \\delta f'(w) + \\frac{\\delta^2}{2}f''(w) + \\dots + (-\\delta)^k f^{(k)} +  o(\\delta^k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\delta$ is usually chosen _small_ so the polynomial is useful for approximiting a small region around some initial $w$. \n",
    "\n",
    "Note: I chose an offset of $-\\delta$, somtimes you will see this written with a positive offset. It does not really matter what the sign is but I find this to be a useful notation in anticipation of th way we will use the series later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We _usually_ find that a _order 2_ or _quadritic_ polynomial is locally close enough to be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often (as is the case with neural networks) a linear approximation to the loss is sufficent as long as we stay within some _trust region_ close to the initial point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import sympy as sp\n",
    "\n",
    "delta = np.linspace(-0.1, 0.1, 20)\n",
    "\n",
    "f = sp.sympify(\"sin(2*pi*w)/2\")\n",
    "eval_f = sp.lambdify(\"w\", f)\n",
    "\n",
    "df = sp.diff(f, \"w\")\n",
    "eval_df = sp.lambdify(\"w\", df)\n",
    "\n",
    "ddf = sp.diff(f, \"w\", \"w\")\n",
    "eval_ddf = sp.lambdify(\"w\", ddf)\n",
    "\n",
    "@interact(w=(0.0, 1.0, 0.001))\n",
    "def plot_approx(w=0.2):\n",
    "    cla()\n",
    "    scatter(w, eval_f(w))\n",
    "    \n",
    "    # The function\n",
    "    plot(linspace(0, 1, 50), eval_f(linspace(0, 1, 50)), ls='--');\n",
    "    \n",
    "    # A Linear Approximation\n",
    "    plot(w-delta, eval_f(w) -delta*eval_df(w), label=\"Linear\")\n",
    "    \n",
    "    # A Quadratic Approximation\n",
    "    plot(w-delta, eval_f(w)  - delta * eval_df(w) + delta**2/2 * eval_ddf(w), label=\"Quadratic\")\n",
    "    \n",
    "    axis('scaled')\n",
    "    ylim(-0.55, 0.55)\n",
    "    legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have functions of _multiple variables_  like $f(w_1, w_2, \\dots, w_n)= f(\\mathbf{w})$, then there is a different _partial derivative_ for each variable\n",
    "\n",
    "$$\\nabla[f] = \\left[\\begin{array} \\\\\n",
    "                      \\frac{\\partial f}{\\partial w_1}\\\\\n",
    "                      \\frac{\\partial f}{\\partial w_2}\\\\\n",
    "                       \\vdots \\\\\n",
    "                      \\frac{\\partial f}{\\partial w_n}\\\\\n",
    "                   \\end{array} \\right]\n",
    "$$\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _nabla_ symbole ($\\nabla$) is used to indicate the _gradient_ of a function, which is the vector of partial derivatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _Jacobian_ is the set of _second derivatives_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J[f] = \n",
    "\\left[\\begin{array} \\\\\n",
    "      \\frac{\\partial^2 f}{\\partial w_1^2} & \\frac{\\partial^2 f}{\\partial w_1 \\partial w_2} & \\cdots &    \\frac{\\partial^2 f}{\\partial w_1 \\partial w_n}\\\\      \n",
    "      \\frac{\\partial^2 f}{\\partial w_1 \\partial w_2} & \\frac{\\partial^2 f}{\\partial w_2^2} & \\cdots &    \\frac{\\partial^2 f}{\\partial w_2 \\partial w_n}\\\\\n",
    "       \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "      \\frac{\\partial^2 f}{\\partial w_1 \\partial w_n} & \\frac{\\partial^2 f}{\\partial w_2 \\partial w_n} & \\cdots & \\frac{\\partial^2 f}{\\partial w_n^2}\\\\\n",
    "\\end{array} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the 2nd order Taylor series becomes\n",
    "\n",
    "$$f(\\mathbf{w}^* - \\pmb{\\epsilon}) = f(\\mathbf{w}^*) - \\pmb{\\epsilon}^T\\Delta[f](\\mathbf{w}^*) + \\frac{1}{2}\\pmb{\\epsilon}^T \\mathbf{J}[f] \\pmb{\\epsilon} + o(||\\pmb{\\epsilon}||^2)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us understand this:\n",
    "1. The vector $\\pmb{\\epsilon}$ is a small random offset from the inital vector $\\pmb{w}^*$. \n",
    "2. The vector $\\Delta[f]$ is the gradient of $f$\n",
    "2. The _matrix_ $J[f]$ is a _symmetric_ matrix of mixed partial derivatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to _minimize_ the function $f(\\mathbf{w})$ then we aim to find some $\\mathbf{w}$ so that $\\Delta[f]$ is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us change variables, so $\\mathbf{A} = \\mathbf{J}[f]$ and $\\mathbf{b}=\\Delta[f]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can rewrite $f(\\mathbf{w}) \\approx f(\\mathbf{w}^*) - \\pmb{\\epsilon}^T \\mathbf{b} + \\frac{1}{2}\\pmb{\\epsilon}^T \\mathbf{A} \\pmb{\\epsilon}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this might only be an approximation for $f$, we can solve for the $\\pmb{\\epsilon}$ that would make its derivative zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f' \\approx -\\mathbf{b} +  \\mathbf{A}\\pmb{\\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  * The derivative $\\frac{\\partial}{\\partial \\epsilon_i} \\left[\\sum_i b_i \\epsilon_i\\right] = b_i$, so $\\Delta[-\\pmb{\\epsilon}^T\\mathbf{b}] = -\\mathbf{b}$ .\n",
    ">  * The derivative \n",
    "$$\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\epsilon_i} \\left[\\pmb{\\epsilon}^T \\mathbf{A}\\pmb{\\epsilon}\\right]\n",
    "&= \\frac{\\partial}{\\partial \\epsilon_i} \\left[\\sum_i \\sum_j A_{i,j} \\epsilon_i \\epsilon_j\\right]\\\\\n",
    "&= \\frac{\\partial}{\\partial \\epsilon_i} \\left[A_{i,j} \\epsilon_i \\epsilon_j + A_{j,i} \\epsilon_i \\epsilon_j\\right] & \\text{(ignore terms without $\\epsilon_i$)}\\\\\n",
    "&= (A_{i,j} + A_{j,i})\\epsilon_j,\n",
    "\\end{align}$$\n",
    "so \n",
    "$$\\begin{align}\n",
    "\\frac{1}{2}\\Delta[\\pmb{\\epsilon}^T \\mathbf{A}\\pmb{\\epsilon}] \n",
    "&= \\frac{1}{2}(\\mathbf{A} + \\mathbf{A}^T)\\pmb{\\epsilon}  \\\\\n",
    "&= \\mathbf{A}\\pmb{\\epsilon} & \\text{(since $\\mathbf{A}=\\mathbf{A}^T$)}\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and when $\\mathbf{A}\\pmb{\\epsilon} = \\mathbf{b}$ we have found a extreme value of the (approximation) to $f$, so we expect $f(\\mathbf{w}^* - \\pmb{\\epsilon})$ to be a minimim of $|\\mathbf{A}|>0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1  Matrix Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous discussion, we expect that one of the main things we need to do is _solve systems of linear equations_ like $\\mathbf{A}\\pmb{\\epsilon} = \\mathbf{b}$ to find offsets ($\\pmb{\\epsilon}$) so that $-\\pmb{\\epsilon}$ points in the (approximate) direction of a minimim of $f$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, we are trying to compute $\\mathbf{A}^{-1}\\mathbf{b}$, however we will find that one _rarely_ wishes to _actually_ calculate $\\mathbf{A}^{-1}$ because it is hard to do so with limite-precision of our computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often know some useful things about $\\mathbf{A}$, especialluy since we know it is a Jacobian:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The matrix $\\mathbf{A}$ for will be _symmetric_ for this type of problem.\n",
    "2. If we can choose $f$ so that $\\left\\|J[f]\\right\\| > 0$ everywhere, then we know that the only stationary points are minima.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common approach to solving $\\mathbf{Ax} = \\mathbf{b}$ is to _factor_ $\\mathbf{A}$ as a product of matrices that are simpler to solve. \n",
    "\n",
    "- Diagonal matrices (often called $\\mathbf{D}$) are _very_ easy to invert; just invert the diagonal entries!\n",
    "- Triangular matrices (zero below or above the diagonal) are easy to solve using backsubstitution or forward substitution. These are often called $\\mathbf{L}$ or $\\mathbf{R}$,or $\\mathbf{U}$ as in \"left right\" or \"lower upper\". \n",
    "- _Orthogonal_ matrices (often called $\\mathbf{Q}$ are very easy to invert since $\\mathbf{Q}^{-1} = \\mathbf{Q}^T$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _Singular Value Decomposition_ (or SVD) is a very useful way to think about a transform. \n",
    "\n",
    "You are **given** :\n",
    " - A list of orthonormal _source directions_ $\\mathbf{v}_1\\dots\\mathbf{v}_m$ as _unit_ vectors (so $\\|\\mathbf{v}_i\\|  = 1$).  These specifiy _direction_ without scale.  These vectors may be part of an $n\\geq m$ dimensional space.\n",
    " - A list of orthonormal _destination_ directions  $\\mathbf{u}_1 \\dots\\mathbf{u}_m$ as _unit_ vectors. \n",
    " - A list of _scales_ $\\sigma_1 \\dots \\sigma_m$ \n",
    " \n",
    "Then the matrix $\\mathbf{A} = \\mathbf{U}\\pmb{\\Sigma}\\mathbf{V}^T$ maps each $\\mathbf{v}_i \\mapsto \\sigma_i\\mathbf{u}_i$. \n",
    "\n",
    "This is a useful way to think about _every_ matrix $\\mathbf{A}$ as a _transformation_ that _rotates_/_reflects_ one orthonormal coordinate system onto another, and also _scales_ it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm, inv\n",
    "\n",
    "from skimage.data import checkerboard, logo\n",
    "from skimage.transform import warp\n",
    "\n",
    "\n",
    "v0 = array([cos(radians(30)), sin(radians(30))])\n",
    "v1 = array([cos(radians(30+90)), sin(radians(30+90))])\n",
    "\n",
    "# A rotation of -15 degrees\n",
    "u0 = array([cos(radians(15)), sin(radians(15))])\n",
    "u1 = array([cos(radians(15+90)), sin(radians(15+90))])\n",
    "\n",
    "# A reflection\n",
    "u1 = -u1\n",
    "\n",
    "# Scales (>= 0)\n",
    "# --> List them in descending order, it is okay to swap them if you also swap U and V columns\n",
    "s0 = 1.4\n",
    "s1 = 0.6\n",
    "\n",
    "\n",
    "U = np.column_stack([u0, u1])\n",
    "Sigma = np.diag([s0, s1])\n",
    "V = np.column_stack([v0, v1])\n",
    "\n",
    "# The SVD\n",
    "A = U.dot(Sigma).dot(V.T)\n",
    "\n",
    "# Homogeneous coordinates (so we can warp an image)\n",
    "A_h = eye(3); A_h[:2,:2] = A\n",
    "im = logo()\n",
    "w, h = im.shape[:2]\n",
    "\n",
    "# Camera matrix for the image\n",
    "view = np.array([[w/2., 0,    w/2.],\n",
    "                 [0,    h/2., h/2.],\n",
    "                 [0,    0,    1]])\n",
    "\n",
    "# Warped image\n",
    "im2 = warp(im, inv(view.dot(A_h.dot(inv(view))))  )\n",
    "\n",
    "subplot(121)\n",
    "title('Source')\n",
    "imshow(im, origin='lower', alpha=0.25, extent=(-1.5, 1.5, -1.5, 1.5))\n",
    "arrow(0,0, *v0, color='g', head_width=0.1, length_includes_head=True)\n",
    "text(*v0, \"$\\mathbf{v}_0$\")\n",
    "arrow(0,0, *v1, color='r', head_width=0.1, length_includes_head=True)\n",
    "text(*v1, \"$\\mathbf{v}_1$\")\n",
    "\n",
    "axis('scaled')\n",
    "xlim(-1.5, 1.5)\n",
    "ylim(-1.5, 1.5)\n",
    "\n",
    "subplot(122)\n",
    "title('Destination')\n",
    "arrow(0,0, *s0*u0, color='g', head_width=0.1, length_includes_head=True)\n",
    "text(*s0*u0, \"$\\mathbf{u}_0$\")\n",
    "text(*s0*u0/2., \"$\\sigma_0$\")\n",
    "\n",
    "arrow(0,0, *s1*u1, color='r', head_width=0.1, length_includes_head=True)\n",
    "text(*s1*u1, \"$\\mathbf{u}_1$\")\n",
    "text(*s1*u1/2., \"$\\sigma_1$\")\n",
    "\n",
    "axis('scaled')\n",
    "xlim(-1.5, 1.5)\n",
    "ylim(-1.5, 1.5)\n",
    "\n",
    "imshow(im2, alpha=0.25, origin='lower', extent=(-1.5, 1.5, -1.5, 1.5))\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "U_, Sigma_, VT_ = svd(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD is not necessarily unique, beut we try to list the columns so that $\\sigma_i$ ar in _descending_ order. Still if it maps $\\mathbf{U}$ to $\\mathbf{V}$ it also maps $\\mathbf{-U}$ to $\\mathbf{-V}$, and vectors with equal $\\sigma$ could be in any order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(U.round(2))\n",
    "print(\"-\"*20)\n",
    "print(U_.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(V.round(2))\n",
    "print(\"-\"*20)\n",
    "print(VT_.T.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diag(Sigma_))\n",
    "print(Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD can be used to compute $\\mathbf{A}^{-1}$ because $\\left(\\mathbf{U}\\times\\text{diag}(\\sigma_i)\\times\\mathbf{V}^T\\right)^{-1}$ = $\\mathbf{V}\\times\\text{diag}(\\frac{1}{\\sigma_i})\\times\\mathbf{U}^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _SVD_ can be hard to compute, and it cannot be computed directly (one must use iterative / nonlinear techniques). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\bf{A}=\\bf{QR}$ decomposition provides a way to factor the matrix into easily invertible matrices $\\mathbf{Q}$ so that $Q^{-1} = Q^T$ and $\\bf{A}^{-1} = \\bf{R}^{-1}\\bf{Q}^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This factorization is useful because $\\bf{Q}$ can be thought of as the axes of a _Cartesian Reference Frame_ and $\\bf{R}$ can be thought of as a _Camera Matrix_ including translation, scaling, and skew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the $\\bf{QR}$ decomposution is numerically robust and can be solved directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import qr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, R = qr(A)\n",
    "print(Q)\n",
    "print(\"-\"*20)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Choleski Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we often have a _symmetric positive definite_ matrix $\\bf{C}$, then it can be expressed as the product $\\bf{R}^2 = \\bf{R}^T\\bf{R}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call $\\bf{R}$ the _square root_ of $\\bf{A}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Choleski decomposition is _fast_ and _robust_ and _easy to compute_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import cholesky\n",
    "\n",
    "C = np.random.rand(5, 5)\n",
    "C = C.T.dot(C) # Make sure it is SPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = cholesky(C)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(R.T.dot(R), C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, the function we want to minimize is a _mean squared error_ between some noisy observations and a parametric function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm # A normal distribution\n",
    "from easydict import EasyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = norm(loc=0, scale=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = 0.5\n",
    "intercept  = 0.4\n",
    "\n",
    "# Naming Convention -- Data, target\n",
    "data = linspace(0, 1, 20)\n",
    "target = slope*data  + intercept + noise.rvs(len(task.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(data, target)\n",
    "plot(data, slope*data  + intercept, ls='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is then to find the parameters to the _best_ line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what does _best_ mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _*correct*_ answer may depend on our (often incomplete) understanding of the noise, and on prior knowledge of what kinds of line are more likely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _common_ answer is the line that to minimize the variance of the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we want to minimize \n",
    "$$ \\sum_{i} \\| t_i - m x_i + b\\|^2$$\n",
    "by solving for unknowns $m, b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write this out using _matrices_ as follows:\n",
    "\n",
    "$$ \\mathbf{A} = \\left[ \\begin{array} \\\\ 1 & x_1 \\\\  1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{array}\\right]$$\n",
    "$$ \\mathbf{x} = \\left[ b, m \\right]^T$$\n",
    "$$ \\mathbf{b} = \\left[ \\begin{array} \\\\  t_1 \\\\  t_2 \\\\ \\vdots  \\\\ t_n \\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we have $\\mathbf{y} = \\mathbf{A}\\mathbf{x}$ and $y_i = b + x_1 \\cdot m$.\n",
    "\n",
    "> I am aware of the confusing notation regarding $\\mathbf{x}$ and $x_i$, just go with it please. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([(1, x) for x in data])\n",
    "x = np.zeros(2)\n",
    "b = np.array(target)\n",
    "print(A.round(2))\n",
    "print(b.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize $\\frac{1}{2}\\| \\mathbf{b} - \\mathbf{Ax} \\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we want to set it's deriviative to zero. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "(b - Ax)^2 &= (b-Ax)^T(b-Ax) \\\\\n",
    "           &= b^T b - b^T Ax - (Ax)^T b - (Ax)^T Ax    & \\text{(distributing)}\\\\\n",
    "           &= b^2 - b^T Ax - x^T A^T b - x^T A^T A x   & \\text{(since $(BC)^T = C^T B^T$) }\\\\\n",
    "           &= b^2 - 2 x^T A^T b - x^T A^T A x          & \\text{(since each term is scaler)}\\\\\n",
    "\\frac{\\partial}{\\partial x_i} (\\cdot) \n",
    "           &= 2 A^T b - 2A^T A x                        & \\text{(see my notes on derivative; $A^T b$ is a vector)} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to set its derivative to zero we must solve\n",
    "$$ A^T A x = A^T b$$\n",
    "which are called the _normal_ equations (they minimize normally distributed error, not sure if that is why)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.T.dot(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.T.dot(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve this one by hand (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, m = solve(A.T.dot(A), A.T.dot(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(data, target)\n",
    "plot(data, A.dot([b, m]))\n",
    "plot(data, A.dot([intercept, slope]))\n",
    "axis('scaled');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **implicit** function for a line is one where the line is where the _function is zero_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implicit functions will often be defined in _one higher dimention_ then tha surfaces they describe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, the implicit function is a _signed distance_, or at least it approximates one near the surface. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a line it is $$ a x + b y + c = 0. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In _Total Least Squares_ we want to minimize the _distance_ from the line (not just the vertical difference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([(x, y, 1) for (x, y) in zip(data, target)])\n",
    "A.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros(3)  # a, b, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we are left with the squared error as minimize $\\| 0 - Ax \\|^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative is zero if $A^TAx = 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly has a minimim at $0$ if $x=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead we must find $x$ so that $A^TAx$ is as small as possible, but $\\|x\\|=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out we know how to do this\n",
    "$$ A^TA = V\\Sigma U^T U \\Sigma v^T = V \\Sigma^2 V^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is minimized for the column of $V$ correponding to the smallest $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, sigma, VT = svd(A)\n",
    "V = VT.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = V[:, np.argmin(sigma)]  # We actually know it is index -1, but I wanted to show you argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = mgrid[0:1:20j, 0:1:20j]\n",
    "d = a*X + b*Y + c\n",
    "\n",
    "contourf(X, Y, exp(-(d**2)/(0.1**2)), levels=linspace(0, 1, 4), cmap=cm.binary_r, alpha=0.5)\n",
    "contour(X, Y, d, levels=0)\n",
    "\n",
    "title(f\"a={a:0.2f}, b={b:0.2f}, c={c:0.2f}\")\n",
    "\n",
    "# Closest points\n",
    "for x, y in zip(data, target):\n",
    "    dist = a*x + b*y +  c\n",
    "    cx = x - a*dist/hypot(a,b)\n",
    "    cy = y - b*dist/hypot(a,b)\n",
    "    plot([x, cx], [y, cy], c='r')\n",
    "\n",
    "scatter(data, target)\n",
    "plot(data, data*slope + intercept)\n",
    "\n",
    "axis('scaled');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we will deal with matrices that are mostly zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will tend to happen when our matrix is an _adjacency matrix_ for a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.grid_2d_graph(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(10,10)\n",
    "nx.draw_networkx(G, pos={key:key for key in G.nodes}, node_size=800, font_size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx = nx.adj_matrix(G)\n",
    "mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
